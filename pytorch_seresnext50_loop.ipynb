{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from data import *\n",
    "import pretrainedmodels\n",
    "import torchvision.transforms as transforms\n",
    "import sys\n",
    "import glob\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "import json\n",
    "from tqdm import tqdm_notebook\n",
    "from senet import *\n",
    "from collections import OrderedDict\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nw = 4   #number of workers for data loader\n",
    "# TOT = 46613580 - 340000\n",
    "\n",
    "NCATS = 340\n",
    "NUM_EACH_CLASS = 30000\n",
    "TOT = NCATS * NUM_EACH_CLASS\n",
    "EPOCHS = 50\n",
    "size = 71\n",
    "SIZE = size # for matching to imagenet\n",
    "batchsize = 4096 \n",
    "lw = 6\n",
    "STEPS = int(NUM_EACH_CLASS * NCATS / EPOCHS / batchsize)\n",
    "\n",
    "channel = 3\n",
    "recognized = True\n",
    "\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.misc \n",
    "try:\n",
    "    from StringIO import StringIO  # Python 2.7\n",
    "except ImportError:\n",
    "    from io import BytesIO         # Python 3.x\n",
    "\n",
    "\n",
    "class Logger(object):\n",
    "    \n",
    "    def __init__(self, log_dir):\n",
    "        \"\"\"Create a summary writer logging to log_dir.\"\"\"\n",
    "        self.writer = tf.summary.FileWriter(log_dir)\n",
    "\n",
    "    def scalar_summary(self, tag, value, step):\n",
    "        \"\"\"Log a scalar variable.\"\"\"\n",
    "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n",
    "        self.writer.add_summary(summary, step)\n",
    "\n",
    "    def image_summary(self, tag, images, step):\n",
    "        \"\"\"Log a list of images.\"\"\"\n",
    "\n",
    "        img_summaries = []\n",
    "        for i, img in enumerate(images):\n",
    "            # Write the image to a string\n",
    "            try:\n",
    "                s = StringIO()\n",
    "            except:\n",
    "                s = BytesIO()\n",
    "            scipy.misc.toimage(img).save(s, format=\"png\")\n",
    "\n",
    "            # Create an Image object\n",
    "            img_sum = tf.Summary.Image(encoded_image_string=s.getvalue(),\n",
    "                                       height=img.shape[0],\n",
    "                                       width=img.shape[1])\n",
    "            # Create a Summary value\n",
    "            img_summaries.append(tf.Summary.Value(tag='%s/%d' % (tag, i), image=img_sum))\n",
    "\n",
    "        # Create and write Summary\n",
    "        summary = tf.Summary(value=img_summaries)\n",
    "        self.writer.add_summary(summary, step)\n",
    "        \n",
    "    def histo_summary(self, tag, values, step, bins=1000):\n",
    "        \"\"\"Log a histogram of the tensor of values.\"\"\"\n",
    "\n",
    "        # Create a histogram using numpy\n",
    "        counts, bin_edges = np.histogram(values, bins=bins)\n",
    "\n",
    "        # Fill the fields of the histogram proto\n",
    "        hist = tf.HistogramProto()\n",
    "        hist.min = float(np.min(values))\n",
    "        hist.max = float(np.max(values))\n",
    "        hist.num = int(np.prod(values.shape))\n",
    "        hist.sum = float(np.sum(values))\n",
    "        hist.sum_squares = float(np.sum(values**2))\n",
    "\n",
    "        # Drop the start of the first bin\n",
    "        bin_edges = bin_edges[1:]\n",
    "\n",
    "        # Add bin edges and counts\n",
    "        for edge in bin_edges:\n",
    "            hist.bucket_limit.append(edge)\n",
    "        for c in counts:\n",
    "            hist.bucket.append(c)\n",
    "\n",
    "        # Create and write Summary\n",
    "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, histo=hist)])\n",
    "        self.writer.add_summary(summary, step)\n",
    "        self.writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEModule(nn.Module):\n",
    "\n",
    "    def __init__(self, channels, reduction):\n",
    "        super(SEModule, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1,\n",
    "                             padding=0)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1,\n",
    "                             padding=0)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        module_input = x\n",
    "        x = self.avg_pool(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return module_input * x\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    \"\"\"\n",
    "    Base class for bottlenecks that implements `forward()` method.\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out = self.se_module(out) + residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class SEBottleneck(Bottleneck):\n",
    "    \"\"\"\n",
    "    Bottleneck for SENet154.\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n",
    "                 downsample=None):\n",
    "        super(SEBottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes * 2)\n",
    "        self.conv2 = nn.Conv2d(planes * 2, planes * 4, kernel_size=3,\n",
    "                               stride=stride, padding=1, groups=groups,\n",
    "                               bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes * 4)\n",
    "        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1,\n",
    "                               bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.se_module = SEModule(planes * 4, reduction=reduction)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "\n",
    "class SEResNetBottleneck(Bottleneck):\n",
    "    \"\"\"\n",
    "    ResNet bottleneck with a Squeeze-and-Excitation module. It follows Caffe\n",
    "    implementation and uses `stride=stride` in `conv1` and not in `conv2`\n",
    "    (the latter is used in the torchvision implementation of ResNet).\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n",
    "                 downsample=None):\n",
    "        super(SEResNetBottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False,\n",
    "                               stride=stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1,\n",
    "                               groups=groups, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.se_module = SEModule(planes * 4, reduction=reduction)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "\n",
    "class SEResNeXtBottleneck(Bottleneck):\n",
    "    \"\"\"\n",
    "    ResNeXt bottleneck type C with a Squeeze-and-Excitation module.\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n",
    "                 downsample=None, base_width=4):\n",
    "        super(SEResNeXtBottleneck, self).__init__()\n",
    "        width = math.floor(planes * (base_width / 64)) * groups\n",
    "        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False,\n",
    "                               stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(width)\n",
    "        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n",
    "                               padding=1, groups=groups, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(width)\n",
    "        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.se_module = SEModule(planes * 4, reduction=reduction)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        \n",
    "class SENet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, groups, reduction, dropout_p=0.2,\n",
    "                 inplanes=128, input_3x3=True, downsample_kernel_size=3,\n",
    "                 downsample_padding=1, num_classes=1000):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        block (nn.Module): Bottleneck class.\n",
    "            - For SENet154: SEBottleneck\n",
    "            - For SE-ResNet models: SEResNetBottleneck\n",
    "            - For SE-ResNeXt models:  SEResNeXtBottleneck\n",
    "        layers (list of ints): Number of residual blocks for 4 layers of the\n",
    "            network (layer1...layer4).\n",
    "        groups (int): Number of groups for the 3x3 convolution in each\n",
    "            bottleneck block.\n",
    "            - For SENet154: 64\n",
    "            - For SE-ResNet models: 1\n",
    "            - For SE-ResNeXt models:  32\n",
    "        reduction (int): Reduction ratio for Squeeze-and-Excitation modules.\n",
    "            - For all models: 16\n",
    "        dropout_p (float or None): Drop probability for the Dropout layer.\n",
    "            If `None` the Dropout layer is not used.\n",
    "            - For SENet154: 0.2\n",
    "            - For SE-ResNet models: None\n",
    "            - For SE-ResNeXt models: None\n",
    "        inplanes (int):  Number of input channels for layer1.\n",
    "            - For SENet154: 128\n",
    "            - For SE-ResNet models: 64\n",
    "            - For SE-ResNeXt models: 64\n",
    "        input_3x3 (bool): If `True`, use three 3x3 convolutions instead of\n",
    "            a single 7x7 convolution in layer0.\n",
    "            - For SENet154: True\n",
    "            - For SE-ResNet models: False\n",
    "            - For SE-ResNeXt models: False\n",
    "        downsample_kernel_size (int): Kernel size for downsampling convolutions\n",
    "            in layer2, layer3 and layer4.\n",
    "            - For SENet154: 3\n",
    "            - For SE-ResNet models: 1\n",
    "            - For SE-ResNeXt models: 1\n",
    "        downsample_padding (int): Padding for downsampling convolutions in\n",
    "            layer2, layer3 and layer4.\n",
    "            - For SENet154: 1\n",
    "            - For SE-ResNet models: 0\n",
    "            - For SE-ResNeXt models: 0\n",
    "        num_classes (int): Number of outputs in `last_linear` layer.\n",
    "            - For all models: 1000\n",
    "        \"\"\"\n",
    "        super(SENet, self).__init__()\n",
    "        self.inplanes = inplanes\n",
    "        if input_3x3:\n",
    "            layer0_modules = [\n",
    "                ('conv1', nn.Conv2d(3, 64, 3, stride=2, padding=1,\n",
    "                                    bias=False)),\n",
    "                ('bn1', nn.BatchNorm2d(64)),\n",
    "                ('relu1', nn.ReLU(inplace=True)),\n",
    "                ('conv2', nn.Conv2d(64, 64, 3, stride=1, padding=1,\n",
    "                                    bias=False)),\n",
    "                ('bn2', nn.BatchNorm2d(64)),\n",
    "                ('relu2', nn.ReLU(inplace=True)),\n",
    "                ('conv3', nn.Conv2d(64, inplanes, 3, stride=1, padding=1,\n",
    "                                    bias=False)),\n",
    "                ('bn3', nn.BatchNorm2d(inplanes)),\n",
    "                ('relu3', nn.ReLU(inplace=True)),\n",
    "            ]\n",
    "        else:\n",
    "            layer0_modules = [\n",
    "                ('conv1', nn.Conv2d(3, inplanes, kernel_size=7, stride=2,\n",
    "                                    padding=3, bias=False)),\n",
    "                ('bn1', nn.BatchNorm2d(inplanes)),\n",
    "                ('relu1', nn.ReLU(inplace=True)),\n",
    "            ]\n",
    "        # To preserve compatibility with Caffe weights `ceil_mode=True`\n",
    "        # is used instead of `padding=1`.\n",
    "        layer0_modules.append(('pool', nn.MaxPool2d(3, stride=2,\n",
    "                                                    ceil_mode=True)))\n",
    "        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))\n",
    "        self.layer1 = self._make_layer(\n",
    "            block,\n",
    "            planes=64,\n",
    "            blocks=layers[0],\n",
    "            groups=groups,\n",
    "            reduction=reduction,\n",
    "            downsample_kernel_size=1,\n",
    "            downsample_padding=0\n",
    "        )\n",
    "        self.layer2 = self._make_layer(\n",
    "            block,\n",
    "            planes=128,\n",
    "            blocks=layers[1],\n",
    "            stride=2,\n",
    "            groups=groups,\n",
    "            reduction=reduction,\n",
    "            downsample_kernel_size=downsample_kernel_size,\n",
    "            downsample_padding=downsample_padding\n",
    "        )\n",
    "        self.layer3 = self._make_layer(\n",
    "            block,\n",
    "            planes=256,\n",
    "            blocks=layers[2],\n",
    "            stride=2,\n",
    "            groups=groups,\n",
    "            reduction=reduction,\n",
    "            downsample_kernel_size=downsample_kernel_size,\n",
    "            downsample_padding=downsample_padding\n",
    "        )\n",
    "        self.layer4 = self._make_layer(\n",
    "            block,\n",
    "            planes=512,\n",
    "            blocks=layers[3],\n",
    "            stride=2,\n",
    "            groups=groups,\n",
    "            reduction=reduction,\n",
    "            downsample_kernel_size=downsample_kernel_size,\n",
    "            downsample_padding=downsample_padding\n",
    "        )\n",
    "        self.avg_pool = nn.AvgPool2d(3, stride=1)\n",
    "        self.dropout = nn.Dropout(dropout_p) if dropout_p is not None else None\n",
    "        self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, groups, reduction, stride=1,\n",
    "                    downsample_kernel_size=1, downsample_padding=0):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=downsample_kernel_size, stride=stride,\n",
    "                          padding=downsample_padding, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, groups, reduction, stride,\n",
    "                            downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups, reduction))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def features(self, x):\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        return x\n",
    "\n",
    "    def logits(self, x):\n",
    "        x = self.avg_pool(x)\n",
    "        if self.dropout is not None:\n",
    "            x = self.dropout(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.last_linear(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.logits(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation function \n",
    "def validation(lossf, scoref):\n",
    "    model.eval()\n",
    "    loss, score = 0, 0\n",
    "    vlen = len(valid_loader)\n",
    "    for x, y in valid_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        output = model(x)\n",
    "        loss += lossf(output, y).item()\n",
    "        score += scoref(output, y)[0].item()\n",
    "    model.train()\n",
    "    return loss/vlen, score/vlen\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_dict = {}\n",
    "path = '../input/train_raw/'\n",
    "\n",
    "filenames = glob.glob(os.path.join(path, '*.csv'))\n",
    "filenames = sorted(filenames)\n",
    "\n",
    "def encode_files():\n",
    "    \"\"\" Encode all label by name of csv_files \"\"\"\n",
    "    counter = 0\n",
    "    for fn in filenames:\n",
    "        en_dict[fn[:-4].split('/')[-1].replace(' ', '_')] = counter\n",
    "        counter += 1\n",
    "        \n",
    "# collect file names and encode label\n",
    "encode_files()\n",
    "\n",
    "dec_dict = {v: k for k, v in en_dict.items()}\n",
    "def decode_labels(label):\n",
    "    return dec_dict[label]\n",
    "\n",
    "def get_label(nfile):\n",
    "    \"\"\" Return encoded label for class by name of csv_files \"\"\"\n",
    "    return en_dict[nfile.replace(' ', '_')[:-4]]\n",
    "\n",
    "def get_label_by_word(word):\n",
    "    return en_dict[word.replace(' ', '_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoodlesDataset(Dataset):\n",
    "    \"\"\"Doodles csv dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, mode='train', nrows=1000, \n",
    "                 skiprows=None, size=256, transform=None, recognized = False):\n",
    "        self.root_dir = root_dir\n",
    "        file = os.path.join(self.root_dir, csv_file)\n",
    "        self.size = size\n",
    "        self.mode = mode\n",
    "        usecols = ['drawing','word'] if mode == 'train' else ['drawing']\n",
    "        if recognized:\n",
    "            usecols += ['recognized']\n",
    "        self.doodle = pd.read_csv(file, usecols=usecols, nrows=nrows, skiprows=skiprows)\n",
    "        if recognized:\n",
    "            self.doodle = self.doodle.loc[self.doodle.recognized == True]\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _draw(raw_strokes, size=256, lw=6, time_color=True):\n",
    "        img = draw_time_encoding(raw_strokes, size = size, lw = lw)\n",
    "        return np.transpose(img, (2, 0, 1))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.doodle)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        raw_strokes = json.loads(self.doodle.drawing.iloc[idx])\n",
    "        sample = self._draw(raw_strokes, size=self.size, lw=6, time_color=True)\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "#         print(sample.shape, sample[None].shape)\n",
    "        if self.mode == 'train':\n",
    "            return (sample/255).astype('float32'), get_label_by_word(self.doodle.word.iloc[idx])\n",
    "        else:\n",
    "            return (sample/255).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/best_se_resnext50_raw_all71_recg.pth\n",
      "Let's use 2 GPUs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kownse/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py:24: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    }
   ],
   "source": [
    "model = SENet(SEResNeXtBottleneck, [3, 4, 6, 3], groups=32, reduction=16,\n",
    "                  dropout_p=None, inplanes=64, input_3x3=False,\n",
    "                  downsample_kernel_size=1, downsample_padding=0,\n",
    "                  num_classes=NCATS) \n",
    "\n",
    "model_prefix = 'se_resnext50_raw_all{}'.format(SIZE)\n",
    "if recognized == True:\n",
    "    model_prefix += '_recg'\n",
    "MODEL_CKPT = 'models/best_{}.pth'.format(model_prefix)\n",
    "print(MODEL_CKPT)\n",
    "\n",
    "last_path= 'models/se_resnext50_detail_end{}.pth'.format(size) \n",
    "# if os.path.exists(last_path):\n",
    "#     print('load from last_path', last_path)\n",
    "#     model.load_state_dict(torch.load(last_path, map_location=lambda storage, loc: storage))\n",
    "# elif os.path.exists(MODEL_CKPT):\n",
    "#     print('load from check point', MODEL_CKPT)\n",
    "#     model.load_state_dict(torch.load(MODEL_CKPT, map_location=lambda storage, loc: storage))\n",
    "# else:\n",
    "#     print('train from scratch')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00005)\n",
    "\n",
    "for idx, child in enumerate(model.children()):\n",
    "    if idx < 5:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "    \n",
    "model.load_state_dict(torch.load('models/best_se_resnext50_raw_all71_8409.pth', map_location=lambda storage, loc: storage)) \n",
    "\n",
    "validationset = DoodlesDataset('valid_raw.csv', \n",
    "                                   root_dir = '../input/', \n",
    "                                   nrows=None, size=SIZE)\n",
    "valid_loader = DataLoader(validationset, batch_size=128, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Validation Loss: 0.6113, Accuracy: 84.10%\n"
     ]
    }
   ],
   "source": [
    "vloss, vscore = validation(criterion, accuracy)\n",
    "print('Start Validation Loss: {:.4f}, Accuracy: {:.2f}%'.format(vloss, vscore))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_skip(lines, select_nrows, end_rows, skip_rows):\n",
    "    if lines < end_rows:\n",
    "        ret = abs(end_rows - lines)\n",
    "        if (lines - ret) < select_nrows:\n",
    "            ret = lines - select_nrows\n",
    "        return ret\n",
    "    else:\n",
    "        return skip_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_round(skip_idx, best_score):\n",
    "    import ntpath\n",
    "    import gc\n",
    "    gc.enable()\n",
    "\n",
    "    select_nrows = NUM_EACH_CLASS\n",
    "    skip_rows = NUM_EACH_CLASS * skip_idx\n",
    "    end_rows = skip_rows + select_nrows\n",
    "\n",
    "    line_cnt = pd.read_csv('../input/line_cnts.csv')\n",
    "    line_cnt['skip_rows'] = line_cnt['line'].apply(lambda x: calc_skip(x, select_nrows, end_rows, skip_rows))\n",
    "    line_cnt['val_skip_rows'] = line_cnt['line'].apply(lambda x : x - 100)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    # collect all single csvset in one\n",
    "\n",
    "    doodle_all = ConcatDataset([DoodlesDataset(fn.split('/')[-1], path, nrows=select_nrows, size=SIZE,\n",
    "                  skiprows=range(1, line_cnt.loc[line_cnt.file == ntpath.split(fn)[1], 'skip_rows'].values[0]),\n",
    "                  recognized = recognized) \n",
    "                                for fn in filenames])\n",
    "    # total images in set\n",
    "    print('Train set:', len(doodle_all))\n",
    "    print('Validation set:', len(validationset))\n",
    "    # Use the torch dataluuoader to iterate through the dataset\n",
    "    loader = DataLoader(doodle_all, batch_size=batchsize, shuffle=True, num_workers=2)\n",
    "\n",
    "    lr_patient = 0\n",
    "    valid_patient = 0\n",
    "    cycle = 0\n",
    "    lr_min = 0.00001\n",
    "    lr_max = 0.001\n",
    "    lr_rampdown = 65\n",
    "\n",
    "    patience = 3\n",
    "    \n",
    "    epochs = 1\n",
    "    lsize = len(loader) \n",
    "    p_itr = int(lsize / 20) # print every N iteration\n",
    "    print('p_itr', p_itr)\n",
    "    itr = 1\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    logger = Logger('./log/pytorch_seresnext50')\n",
    "    step = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        tloss, score = 0, 0\n",
    "        for x, y in tqdm_notebook(loader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tloss += loss.item()\n",
    "            score += accuracy(output, y)[0].item()\n",
    "\n",
    "            if itr%p_itr==0:\n",
    "                vloss, vscore = validation(criterion, accuracy)\n",
    "                print('Iteration {} -> Train Loss: {:.4f}, Accuracy: {:.2f}%'.format(itr, tloss/p_itr, score/p_itr))\n",
    "                print('Iteration {} -> Validation Loss: {:.4f}, Accuracy: {:.2f}%'.format(itr, vloss, vscore))\n",
    "                print('next validation:', itr + p_itr)\n",
    "                time.sleep(0.1)\n",
    "                \n",
    "                info = { 'loss': tloss/p_itr, 'categorical_accuracy': score/p_itr, 'val_loss': vloss, 'val_categorical_accuracy': vscore }\n",
    "                for tag, value in info.items():\n",
    "                    logger.scalar_summary(tag, value, step+1)\n",
    "                step += 1\n",
    "                \n",
    "                if vscore >= best_score:\n",
    "                    lr_patient = 0\n",
    "                    valid_patient = 0\n",
    "                    best_score = vscore\n",
    "\n",
    "                    model.eval()\n",
    "                    torch.save(model.state_dict(), MODEL_CKPT)\n",
    "                    model.train()\n",
    "\n",
    "                    print('new best_score{:.3f}'.format(vscore), 'save to', MODEL_CKPT)\n",
    "                else:\n",
    "                    lr_patient += 1\n",
    "                    valid_patient += 1\n",
    "\n",
    "                if valid_patient >=  15:\n",
    "                    cycle += 1\n",
    "                    lr_patient = 0\n",
    "                    valid_patient = 0\n",
    "\n",
    "                    checkpath = 'models/se_resnext50_parts_cycle{}.pth'.format(cycle)\n",
    "                    model.eval()\n",
    "                    torch.save(model.state_dict(), checkpath)\n",
    "                    model.train()\n",
    "\n",
    "                    print('rampdown')\n",
    "                    for params in optimizer.param_groups:\n",
    "                        params['lr'] = (lr_min + 0.5 * (lr_max - lr_min) *\n",
    "                                       (1 + np.cos(np.pi * step / lr_rampdown)))\n",
    "\n",
    "                    print('Learning rate set to {:.4}'.format(optimizer.param_groups[0]['lr']))\n",
    "\n",
    "                if lr_patient >= patience:\n",
    "                    print('Reducing learning rate by {}'.format(0.5))\n",
    "                    for params in optimizer.param_groups:\n",
    "                        params['lr'] *= 0.5\n",
    "                    lr_patient = 0\n",
    "\n",
    "                tloss, score = 0, 0 \n",
    "\n",
    "            itr +=1\n",
    "    vloss, vscore = validation(criterion, accuracy)\n",
    "    print('Final -> Validation  Loss: {:.4f}, Accuracy: {:.2f}%'.format(vloss, vscore))\n",
    "\n",
    "    model.eval()\n",
    "    torch.save(model.state_dict(), last_path)\n",
    "    \n",
    "    del loader\n",
    "    del doodle_all\n",
    "    gc.collect()\n",
    "    \n",
    "    return best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_score = 0\n",
    "\n",
    "for i in range(0, 10): \n",
    "    best_score = one_round(i, best_score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(MODEL_CKPT, map_location=lambda storage, loc: storage))\n",
    "model = model.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = DoodlesDataset('test_raw.csv', '../input', mode='test', nrows=None, size=SIZE)\n",
    "testloader = DataLoader(testset, batch_size=256, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae06f7a54f1b480eabe7717f313f51bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=439), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "labels = np.empty((0,3))\n",
    "for x in tqdm_notebook(testloader):\n",
    "    x = x.to(device)\n",
    "    output = model(x)\n",
    "    _, pred = output.topk(3, 1, True, True)\n",
    "    labels = np.concatenate([labels, pred], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add91b1244a24351986fd690813458d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=112199), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "submission = pd.read_csv('../input/test_raw.csv', index_col='key_id')\n",
    "submission.drop(['countrycode', 'drawing'], axis=1, inplace=True)\n",
    "submission['word'] = ''\n",
    "prog = tqdm_notebook(total = len(submission))\n",
    "for i, label in enumerate(labels):\n",
    "    prog.update(1)\n",
    "    submission.word.iloc[i] = \" \".join([decode_labels(l) for l in label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save result\n",
      "upload result\n",
      "cmd: kaggle competitions submit -c quickdraw-doodle-recognition -f ../result/se_resnext50_raw_all71_recg.csv.7z -m \"submit\"\n"
     ]
    }
   ],
   "source": [
    "import kaggle_util\n",
    "kaggle_util.save_result(submission,  \n",
    "                        '../result/{}.csv'.format(model_prefix), \n",
    "                         'quickdraw-doodle-recognition', \n",
    "                        send=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([71, 3, 71, 71])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0052df6b728147d1867af5c6f1e07c0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=439), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(112199, 340)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pres = np.empty((0,340))\n",
    "for x in tqdm_notebook(testloader):\n",
    "    x = x.to(device)\n",
    "    output = model(x)\n",
    "    pres = np.concatenate([pres, output.detach().cpu().numpy()], axis = 0)\n",
    "    \n",
    "pres.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "swaps = [\n",
    "    (18,19),\n",
    "    (130,129),\n",
    "    (156,155),\n",
    "]\n",
    "\n",
    "for pair in swaps:\n",
    "    temp = np.copy(pres[:,pair[0]])\n",
    "    pres[:,pair[0]] = pres[:,pair[1]]\n",
    "    pres[:,pair[1]] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_softmax = softmax(pres.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../result/pre_senext_8410.npy', pre_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../result/pre_senext_8410.npy'.format(model_prefix), pre_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
