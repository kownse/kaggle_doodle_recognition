{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Activation\n",
    "from keras.models import Model\n",
    "\n",
    "from glob import glob\n",
    "import json\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "from data import *\n",
    "from keras.applications import Xception\n",
    "from keras.applications.xception import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 71\n",
    "batchsize = 192\n",
    "lw = 6\n",
    "channel = 3\n",
    "\n",
    "STROKE_COUNT = 100\n",
    "NCATS = 340\n",
    "TOT = 46613580 - 340000\n",
    "EPOCHS = 70\n",
    "STEPS = TOT / EPOCHS / batchsize \n",
    "\n",
    "conv_filters = 64\n",
    "lstm_units = 384\n",
    "lstm_cnt = 3\n",
    "dropout = 0.2\n",
    "\n",
    "model_prefix = 'stack_xception_lstm_conv{}_units{}_layer{}_dropout{}_feats9'.format(conv_filters, lstm_units, lstm_cnt, dropout)\n",
    "print(model_prefix)\n",
    "check_path = 'models/best_{}.hdf5'.format(model_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preds2catids(predictions):\n",
    "    return pd.DataFrame(np.argsort(-predictions, axis=1)[:, :3], columns=['a', 'b', 'c'])\n",
    "\n",
    "def top_3_accuracy(y_true, y_pred):\n",
    "    return top_k_categorical_accuracy(y_true, y_pred, k=3)\n",
    "\n",
    "def unit_vector(vector):\n",
    "    \"\"\" Returns the unit vector of the vector.  \"\"\"\n",
    "    return vector / np.linalg.norm(vector)\n",
    "\n",
    "def angle_between(v1, v2):\n",
    "    \"\"\" Returns the angle in radians between vectors 'v1' and 'v2'::\n",
    "\n",
    "            >>> angle_between((1, 0, 0), (0, 1, 0))\n",
    "            1.5707963267948966\n",
    "            >>> angle_between((1, 0, 0), (1, 0, 0))\n",
    "            0.0\n",
    "            >>> angle_between((1, 0, 0), (-1, 0, 0))\n",
    "            3.141592653589793\n",
    "    \"\"\"\n",
    "    if (v1[0] == 0 and v1[1] == 0) or (v2[0] == 0 and v2[1] == 0):\n",
    "        return 0\n",
    "    \n",
    "    v1_u = unit_vector(v1)\n",
    "    v2_u = unit_vector(v2)\n",
    "    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n",
    "\n",
    "def intervaled_cumsum(ar, new_flag):\n",
    "    # Make a copy to be used as output array\n",
    "    out = ar.copy()\n",
    "\n",
    "    cumsum = 0\n",
    "    for i in range(len(ar)):\n",
    "        if new_flag[i] == 2:\n",
    "            cumsum = 0\n",
    "            out[i] = cumsum\n",
    "        else:\n",
    "            cumsum += ar[i]\n",
    "            out[i] = cumsum\n",
    "\n",
    "    return out\n",
    "\n",
    "def _stack_it(raw_strokes):\n",
    "    \"\"\"preprocess the string and make \n",
    "    a standard Nx3 stroke vector\"\"\"\n",
    "    stroke_vec = json.loads(raw_strokes) # string->list\n",
    "\n",
    "    # unwrap the list\n",
    "    in_strokes = [(xi,yi,i) for i,(x,y) in enumerate(stroke_vec) for xi,yi in zip(x,y)]\n",
    "    c_strokes = np.stack(in_strokes)\n",
    "    \n",
    "    # replace stroke id with 1 for continue, 2 for new\n",
    "    c_strokes[:,2] += 1 # since 0 is no stroke\n",
    "    new_flag = np.array([1]+np.diff(c_strokes[:,2]).tolist()) + 1\n",
    "\n",
    "    # calc distance \n",
    "    x_diff = [0] + np.diff(c_strokes[:,0]).tolist()\n",
    "    y_diff = [0] + np.diff(c_strokes[:,1]).tolist()\n",
    "    distance = np.sqrt(np.power(x_diff, 2) + np.power(y_diff, 2)).astype(np.uint32)\n",
    "    \n",
    "    # calc length for one stroke\n",
    "    length = np.bincount(c_strokes[:,2], weights=distance).astype(np.uint32)\n",
    "    leng = np.zeros_like(distance)\n",
    "    for i in range(1, len(length)):\n",
    "        leng[c_strokes[:,2] == i] = length[i]\n",
    "    \n",
    "    c_strokes = np.column_stack((c_strokes, new_flag, distance, leng))\n",
    "    c_strokes[c_strokes[:,3] == 2,4] = 0\n",
    "    \n",
    "    len_cumsum = intervaled_cumsum(c_strokes[:,4],c_strokes[:,2])  \n",
    "    c_strokes = np.column_stack((c_strokes, len_cumsum))\n",
    "\n",
    "    # pad the strokes with zeros\n",
    "    return pad_sequences(c_strokes.swapaxes(0, 1), \n",
    "                         maxlen=STROKE_COUNT, \n",
    "                         padding='post').swapaxes(0, 1)\n",
    "\n",
    "def _stack_it9_vec(stroke_vec):\n",
    "    for d in stroke_vec:\n",
    "        angle = []\n",
    "        for i in range(len(d[0])):\n",
    "            if i < 2:\n",
    "                angle.append(0)\n",
    "            else:\n",
    "                v1 = (d[0][i-1] - d[0][i-2],d[1][i-1] - d[1][i-2])\n",
    "                v2 = (d[0][i] - d[0][i-1],d[1][i] - d[1][i-1])\n",
    "                a = angle_between(v1,v2)\n",
    "                a = int(a)\n",
    "                angle.append(a)\n",
    "\n",
    "        d.append(angle)\n",
    "\n",
    "    # unwrap the list\n",
    "    in_strokes = [(xi,yi,ai,i) for i,(x,y,a) in enumerate(stroke_vec) for xi,yi,ai in zip(x,y,a)]\n",
    "    c_strokes = np.stack(in_strokes)\n",
    "\n",
    "    # replace stroke id with 1 for continue, 2 for new\n",
    "    c_strokes[:,3] += 1 # since 0 is no stroke\n",
    "    new_flag = np.array([1]+np.diff(c_strokes[:,3]).tolist()) + 1\n",
    "\n",
    "    # calc distance \n",
    "    x_diff = [0] + np.diff(c_strokes[:,0]).tolist()\n",
    "    y_diff = [0] + np.diff(c_strokes[:,1]).tolist()\n",
    "    distance = np.sqrt(np.power(x_diff, 2) + np.power(y_diff, 2)).astype(np.uint32)\n",
    "\n",
    "    # calc length for one stroke\n",
    "    length = np.bincount(c_strokes[:,3], weights=distance).astype(np.uint32)\n",
    "    leng = np.zeros_like(distance)\n",
    "    for i in range(1, len(length)):\n",
    "        leng[c_strokes[:,3] == i] = length[i]\n",
    "\n",
    "    c_strokes = np.column_stack((c_strokes, new_flag, distance, leng))\n",
    "    c_strokes[c_strokes[:,4] == 2,5] = 0\n",
    "\n",
    "    angle_cumsum = intervaled_cumsum(c_strokes[:,2],c_strokes[:,4])\n",
    "    length_cumsum = intervaled_cumsum(c_strokes[:,5],c_strokes[:,4])\n",
    "    c_strokes = np.column_stack((c_strokes, angle_cumsum, length_cumsum))\n",
    "\n",
    "    # pad the strokes with zeros\n",
    "    return pad_sequences(c_strokes.swapaxes(0, 1), \n",
    "                         maxlen=STROKE_COUNT, \n",
    "                         padding='post').swapaxes(0, 1)\n",
    "\n",
    "def _stack_it9(raw_strokes):\n",
    "    stroke_vec = json.loads(raw_strokes)\n",
    "    return _stack_it9_vec(stroke_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_df = pd.read_csv('../input/valid.csv', nrows=10)\n",
    "# _stack_it(val_df.iloc[1]['drawing'])[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_generator_xd(batchsize, df_path = '../input/train_all.csv'):\n",
    "    while True:\n",
    "        for df in pd.read_csv(df_path, chunksize=batchsize):\n",
    "            df['drawing'] = df['drawing'].apply(json.loads)\n",
    "            x2 = np.stack(df['drawing'].apply(_stack_it9_vec), 0)\n",
    "            \n",
    "            x1 = np.zeros((len(df), size, size, 3), dtype=np.uint8)\n",
    "            for i, raw_strokes in enumerate(df.drawing.values):\n",
    "                x1[i, :, :, :] = draw_cv2_parts(raw_strokes, size=size, \n",
    "                                             lw=lw, center = False)\n",
    "            \n",
    "            y = keras.utils.to_categorical(df['word'], num_classes=NCATS)\n",
    "            yield {'batch_normalization_1_input': x2, 'input_1': x1}, y\n",
    "\n",
    "def df_to_image_array_xd(df):\n",
    "    df['drawing'] = df['drawing'].apply(json.loads)\n",
    "    x2 = np.stack(df['drawing'].apply(_stack_it9_vec), 0)\n",
    "    \n",
    "    x1 = np.zeros((len(df), size, size, 3), dtype=np.uint8)\n",
    "    for i, raw_strokes in enumerate(df.drawing.values):\n",
    "        x1[i, :, :, :] = draw_cv2_parts(raw_strokes, size=size, \n",
    "                                     lw=lw, center = False)\n",
    "\n",
    "    return {'batch_normalization_1_input': x2, 'input_1': x1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization, Conv1D, Dense, Dropout, Bidirectional\n",
    "from keras.layers import CuDNNLSTM\n",
    "\n",
    "stroke_read_model = Sequential()\n",
    "stroke_read_model.add(BatchNormalization(input_shape = (None,)+(9,)))\n",
    "stroke_read_model.add(Conv1D(conv_filters, (5,), activation = 'relu'))\n",
    "stroke_read_model.add(Conv1D(conv_filters*2, (5,), activation = 'relu'))\n",
    "stroke_read_model.add(Conv1D(conv_filters*4, (3,), activation = 'relu'))\n",
    "for i in range(lstm_cnt - 1):\n",
    "    print('return_sequences', i != (lstm_cnt - 1))\n",
    "    stroke_read_model.add(Bidirectional(CuDNNLSTM(lstm_units, return_sequences = True)))\n",
    "stroke_read_model.add(Bidirectional(CuDNNLSTM(lstm_units, return_sequences = False)))\n",
    "                                    \n",
    "stroke_read_model.add(Dense(512, activation = 'relu'))\n",
    "stroke_read_model.add(Dropout(dropout))\n",
    "stroke_read_model.add(Dense(NCATS, activation = 'softmax'))\n",
    "\n",
    "stroke_read_model.load_weights('models/best_lstm_conv64_units384_layer3_dropout0.2_feats9_noconvdrop.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xception_model = Xception(input_shape=(None,None,3), weights=None, classes=NCATS)\n",
    "xception_model.load_weights('models/xception71_parts_lw6_balance_0_adam_color.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_input = stroke_read_model.layers[0].input\n",
    "lstm_out = stroke_read_model.layers[len(stroke_read_model.layers) - 3].output\n",
    "\n",
    "\n",
    "cnn_input = xception_model.layers[0].input\n",
    "cnn_out = xception_model.layers[len(xception_model.layers)-2].output\n",
    "\n",
    "print(lstm_out)\n",
    "print(cnn_out)\n",
    "x = keras.layers.concatenate([lstm_out, cnn_out])\n",
    "\n",
    "# x = Dense(512)(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Activation('relu')(x)\n",
    "\n",
    "x = Dense(128)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "final_out = Dense(NCATS, activation = 'softmax')(x)\n",
    "\n",
    "model = Model(inputs=[lstm_input, cnn_input], outputs=[final_out])\n",
    "\n",
    "for i in range(len(model.layers)-4):\n",
    "    model.layers[i].trainable = False\n",
    "\n",
    "model.compile(optimizer = Adam(lr=0.002), \n",
    "                          loss = 'categorical_crossentropy', \n",
    "                          metrics = ['categorical_accuracy', top_3_accuracy])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = image_generator_xd(batchsize=batchsize, df_path = '../input/train_all.csv')\n",
    "val_df = pd.read_csv('../input/valid.csv')\n",
    "x_val = df_to_image_array_xd(val_df)\n",
    "y_val = keras.utils.to_categorical(val_df.word, num_classes=NCATS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(check_path, monitor='val_categorical_accuracy', verbose=1, \n",
    "                             save_best_only=True, mode='max', save_weights_only = True)\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.5, patience=5, \n",
    "                                   verbose=1, mode='max', epsilon=0.0001, cooldown=5, min_lr=0.0001)\n",
    "early = EarlyStopping(monitor=\"val_categorical_accuracy\", \n",
    "                      mode=\"max\", \n",
    "                      patience=20) \n",
    "board =TensorBoard(log_dir='./log/{}'.format(model_prefix))\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat, board]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit_generator(train_datagen, steps_per_epoch=STEPS, epochs=EPOCHS, verbose=1,\n",
    "                        validation_data=(x_val, y_val),\n",
    "                      callbacks = callbacks_list)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/test_simplified.csv')\n",
    "x_test = df_to_image_array_xd(test)\n",
    "print(test.shape, x_test.shape)\n",
    "print('Test array memory {:.2f} GB'.format(x_test.nbytes / 1024.**3 ))\n",
    "\n",
    "np_classes = np.load('../input/classes.npy')\n",
    "id2cat = {k: cat.replace(' ', '_') for k, cat in enumerate(np_classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def doodle_predict(model, model_path, x_test):\n",
    "    model.load_weights(model_path)\n",
    "\n",
    "    test_predictions = model.predict(x_test, batch_size=512, verbose=1)\n",
    "    top3 = preds2catids(test_predictions)\n",
    "    top3cats = top3.replace(id2cat)\n",
    "    test['word'] = top3cats['a'] + ' ' + top3cats['b'] + ' ' + top3cats['c']\n",
    "    submission = test[['key_id', 'word']]\n",
    "\n",
    "    import kaggle_util\n",
    "    kaggle_util.save_result(submission,  \n",
    "                            '../result/{}.csv'.format(model_prefix), \n",
    "                            'quickdraw-doodle-recognition', \n",
    "                            send=True, index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doodle_predict(stroke_read_model, check_path, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = stroke_read_model.predict(x_test, batch_size=512, verbose=1)\n",
    "np.save('../result/{}.npy'.format(model_prefix), test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
